{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет, меня зовут Артем. Сегодня я проверю твой проект.\n",
    "<br> Дальнейшее общение будет происходить на \"ты\" если это не вызывает никаких проблем.\n",
    "<br> Желательно реагировать на каждый мой комментарий ('исправил', 'не понятно как исправить ошибку', ...)\n",
    "<br> Пожалуйста, не удаляй комментарии ревьюера, так как они повышают качество повторного ревью.\n",
    "\n",
    "Комментарии будут в <font color='green'>зеленой</font>, <font color='blue'>синей</font> или <font color='red'>красной</font> рамках:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Если все сделано отлично\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Если можно немного улучшить\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b> Если требуются исправления. Работа не может быть принята с красными комментариями.\n",
    "</div>\n",
    "\n",
    "-------------------\n",
    "\n",
    "Будет очень хорошо, если ты будешь помечать свои действия следующим образом:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий студента:</b> ...\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения ...\n",
    "</div>\n",
    "\n",
    "<font color='green'><b>Полезные (и просто интересные) материалы:</b></font> \\\n",
    "Для работы с текстами используют и другие подходы. Например, сейчас активно используются RNN (LSTM) и трансформеры (BERT и другие с улицы Сезам, например, ELMO). НО! Они не являются панацеей, не всегда они нужны, так как и TF-IDF или Word2Vec + модели из классического ML тоже могут справляться. \\\n",
    "BERT тяжелый, существует много его вариаций для разных задач, есть готовые модели, есть надстройки над библиотекой transformers. Если, обучать BERT на GPU (можно в Google Colab или Kaggle), то должно быть побыстрее.\\\n",
    "https://huggingface.co/transformers/model_doc/bert.html \\\n",
    "https://t.me/renat_alimbekov \\\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - Про LSTM \\\n",
    "https://web.stanford.edu/~jurafsky/slp3/10.pdf - про энкодер-декодер модели, этеншены\\\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html - официальный гайд\n",
    "по трансформеру от создателей pytorch\\\n",
    "https://transformer.huggingface.co/ - поболтать с трансформером \\\n",
    "Библиотеки: allennlp, fairseq, transformers, tensorflow-text — множествореализованных\n",
    "методов для трансформеров методов NLP \\\n",
    "Word2Vec https://radimrehurek.com/gensim/models/word2vec.html \n",
    "\n",
    "<font color='green'>Пример BERT с GPU:\n",
    "```python\n",
    "%%time\n",
    "from tqdm import notebook\n",
    "batch_size = 2 # для примера возьмем такой батч, где будет всего две строки датасета\n",
    "embeddings = [] \n",
    "for i in notebook.tqdm(range(input_ids.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(input_ids[batch_size*i:batch_size*(i+1)]).cuda() # закидываем тензор на GPU\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.cuda()\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy()) # перевод обратно на проц, чтобы в нумпай кинуть\n",
    "        del batch\n",
    "        del attention_mask_batch\n",
    "        del batch_embeddings\n",
    "        \n",
    "features = np.concatenate(embeddings) \n",
    "```\n",
    "Можно сделать предварительную проверку на наличие GPU.\\\n",
    "Например, так: ```device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")```\\\n",
    "Тогда вместо .cuda() нужно писать .to(device)\n",
    "\n",
    "Если понравилась работа с текстами, то можешь посмотреть очень интересный (но очень-очень сложный) курс лекций: https://github.com/yandexdataschool/nlp_course .\n",
    "</font>\n",
    "\n",
    "### <font color='orange'>Общее впечатление</font>\n",
    "* Большое спасибо за проделанную работу. Видно, что приложено много усилий.\n",
    "* Радует, что ноутбук хорошо структурирован. Приятно проверять такие работы.\n",
    "* Отлично, что стоп-слова были исключены!\n",
    "* Над этим проектом нужно будет еще немного поработать. Однако, изменения не должны занять много времени.\n",
    "* С радостью отвечу на твои вопросы, если они есть. Лучше всего их собрать в следующей ячейке. Жду новую версию проекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>Общее впечатление (ревью 2)</font>\n",
    "* Для удобства все новые комментарии обозначены фразой \"ревью 2\".\n",
    "* Удачи в доработке!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>Общее впечатление (ревью 3)</font>\n",
    "* После исправлений проект улучшился и теперь он может быть зачтен.\n",
    "* Удачи в дальнейшем обучении и следующих работах!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Общее-впечатление\" data-toc-modified-id=\"Общее-впечатление-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-2)\" data-toc-modified-id=\"Общее-впечатление-(ревью-2)-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление (ревью 2)</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-3)\" data-toc-modified-id=\"Общее-впечатление-(ревью-3)-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление (ревью 3)</font></a></span></li></ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Импорт-библиотек\" data-toc-modified-id=\"Импорт-библиотек-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Импорт библиотек</a></span></li><li><span><a href=\"#Загрузка-и-подготовка-данных\" data-toc-modified-id=\"Загрузка-и-подготовка-данных-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Загрузка и подготовка данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Подготовка-признаков-перед-обучением\" data-toc-modified-id=\"Подготовка-признаков-перед-обучением-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Подготовка признаков перед обучением</a></span><ul class=\"toc-item\"><li><span><a href=\"#Очистка-и-лемматизация\" data-toc-modified-id=\"Очистка-и-лемматизация-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Очистка и лемматизация</a></span></li><li><span><a href=\"#Stopwords,-TF-IDF\" data-toc-modified-id=\"Stopwords,-TF-IDF-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Stopwords, TF-IDF</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#DecisionTreeClassifier\" data-toc-modified-id=\"DecisionTreeClassifier-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>DecisionTreeClassifier</a></span></li><li><span><a href=\"#Тестирование-модели\" data-toc-modified-id=\"Тестирование-модели-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Тестирование модели</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Отлично, что все импорты собраны в первой ячейке ноутбука! Если у того, кто будет запускать твой ноутбук будут отсутствовать некоторые библиотеки, то он это увидит сразу, а не в процессе!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df=pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df=pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.841344371679229"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ratio = df['toxic'].value_counts()[0] / df['toxic'].value_counts()[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Данные загружены корреткно. Радует, что баланс классов был изучен.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы, удалим столбец `Unnamed: 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes={0:1, 1:class_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка признаков перед обучением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Очистка и лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    cleared_text = re.sub(r'(?:\\n|\\r)', ' ', cleared_text)\n",
    "    return \" \".join(cleared_text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,               \n",
    "                \"N\": wordnet.NOUN,              \n",
    "                \"V\": wordnet.VERB,              \n",
    "                \"R\": wordnet.ADV                \n",
    "               }  \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "    return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 26s, sys: 1min 28s, total: 16min 55s\n",
      "Wall time: 16min 56s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of ask when your view ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that be a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it look like it be actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>and i really don t think you understand i came...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really don t think you understand i come...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       explanation why the edits made under my userna...      0   \n",
       "1       d aww he matches this background colour i m se...      0   \n",
       "2       hey man i m really not trying to edit war it s...      0   \n",
       "3       more i can t make any real suggestions on impr...      0   \n",
       "4       you sir are my hero any chance you remember wh...      0   \n",
       "...                                                   ...    ...   \n",
       "159287  and for the second time of asking when your vi...      0   \n",
       "159288  you should be ashamed of yourself that is a ho...      0   \n",
       "159289  spitzer umm theres no actual article for prost...      0   \n",
       "159290  and it looks like it was actually you who put ...      0   \n",
       "159291  and i really don t think you understand i came...      0   \n",
       "\n",
       "                                                lemm_text  \n",
       "0       explanation why the edits make under my userna...  \n",
       "1       d aww he match this background colour i m seem...  \n",
       "2       hey man i m really not try to edit war it s ju...  \n",
       "3       more i can t make any real suggestion on impro...  \n",
       "4       you sir be my hero any chance you remember wha...  \n",
       "...                                                   ...  \n",
       "159287  and for the second time of ask when your view ...  \n",
       "159288  you should be ashamed of yourself that be a ho...  \n",
       "159289  spitzer umm there no actual article for prosti...  \n",
       "159290  and it look like it be actually you who put on...  \n",
       "159291  and i really don t think you understand i come...  \n",
       "\n",
       "[159292 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['lemm_text'] = df['text'].apply(lemmatize_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Очистка была сделана верно.\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b> Этот лемматизатор работает только с русским языком, так что его нет смысла применять. Я бы использовал WordNetLemmatizer. Отмечу, что его нужно применять к словам, а не ко всему тексту сразу.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Ошибка:</b> WordNetLemmatizer лемматизатор нужно применять с соответствующим POS-тегом. Этот процесс подробно описан в абзаце \"Wordnet Lemmatizer с соответствующим POS-тегом\" в статье: https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Комментарий студента:</b> Исправлено.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения: исправлена функция лемматизации\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка (ревью 2):</b> Я не увидел применения POS-тега.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Комментарий студента(ревью 2):</b> Исправлено.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   toxic      159292 non-null  int64 \n",
      " 1   lemm_text  159292 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['text'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем выборку на тренировочное, валидационное и тестовое множества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['toxic']\n",
    "features = df.drop(['toxic'], axis=1)\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.4, \n",
    "                                                                              random_state=12345)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=0.5,\n",
    "                                                                            random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95575, 1)\n",
      "(31858, 1)\n",
      "(31859, 1)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopwords, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'])\n",
    "features_valid = count_tf_idf.transform(features_valid['lemm_text'])\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Разбиение было сделано верно. Отлично, что векторизатор был обучен только на тренировочной части данных.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b> Приводить тексты к юникоду не имеет смысла, так как они все на английском. Это может привести к падению ядра из-за увеличения объема занимаемой памяти.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Комментарий студента:</b> Исправлено.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения: удалено приведение текстов к юникоду\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех (ревью 2):</b> ОК.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения будем использовать модели:\n",
    "\n",
    "    LogisticRegression\n",
    "    DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_counts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters\n",
      "\n",
      "Best parameters:\n",
      "\n",
      "{'C': 10, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'lbfgs'}\n",
      "\n",
      "Grid scores:\n",
      "\n",
      "0.707272 for {'C': 0.1, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'newton-cg'}\n",
      "0.707304 for {'C': 0.1, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'lbfgs'}\n",
      "0.707080 for {'C': 0.1, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'liblinear'}\n",
      "0.746550 for {'C': 1, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'newton-cg'}\n",
      "0.746505 for {'C': 1, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'lbfgs'}\n",
      "0.746445 for {'C': 1, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'liblinear'}\n",
      "0.751718 for {'C': 10, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'newton-cg'}\n",
      "0.752547 for {'C': 10, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'lbfgs'}\n",
      "0.751755 for {'C': 10, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'liblinear'}\n",
      "\n",
      "CPU times: user 4min 34s, sys: 5min 54s, total: 10min 29s\n",
      "Wall time: 10min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "hyperparams = [{'solver':['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'C':[0.1, 1, 10],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters')\n",
    "print()\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=cv_counts)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parameters:\")\n",
    "print()\n",
    "LR_best_params = clf.best_params_\n",
    "print(LR_best_params)\n",
    "print()\n",
    "print(\"Grid scores:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_LR = max(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Напомню, что внутри кросс-валидации происходит разбиение выборки на треин и валидацию. Однако, в таком случае векторизатор обучен на всей выборке, а это не совсем корректно. Для избежания такого эффекта можно использовать <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">пайплайн</a> из векторизатора и модели. <a href=\"https://medium.com/analytics-vidhya/ml-pipelines-using-scikit-learn-and-gridsearchcv-fe605a7f9e05\">Тут</a> есть пример.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal F1 0.7525472577805025\n",
      "Валидация F1 0.7574939622105413\n",
      "CPU times: user 19 s, sys: 23.3 s, total: 42.3 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_LR = f1_score(target_valid, target_predict)\n",
    "print('CrossVal F1', cv_f1_LR)\n",
    "print('Валидация F1', valid_f1_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters\n",
      "\n",
      "Best parameters:\n",
      "\n",
      "{'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 45, 'random_state': 12345}\n",
      "\n",
      "Grid scores:\n",
      "\n",
      "0.583522 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 10, 'random_state': 12345}\n",
      "0.608500 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 15, 'random_state': 12345}\n",
      "0.609225 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 20, 'random_state': 12345}\n",
      "0.620805 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 25, 'random_state': 12345}\n",
      "0.616930 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 30, 'random_state': 12345}\n",
      "0.616370 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 35, 'random_state': 12345}\n",
      "0.612446 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 40, 'random_state': 12345}\n",
      "0.620953 for {'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 45, 'random_state': 12345}\n",
      "\n",
      "CPU times: user 5min 16s, sys: 3.31 s, total: 5min 20s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = DecisionTreeClassifier()\n",
    "hyperparams = [{'max_depth':[x for x in range(10,50,5)],\n",
    "                'random_state':[12345],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters')\n",
    "print()\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=cv_counts)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parameters:\")\n",
    "print()\n",
    "DTC_best_params = clf.best_params_\n",
    "print(DTC_best_params)\n",
    "print()\n",
    "print(\"Grid scores:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_DTC = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal F1 0.620952517141258\n",
      "Валидация F1 0.6226754649070186\n",
      "CPU times: user 25.3 s, sys: 215 ms, total: 25.5 s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = DecisionTreeClassifier()\n",
    "classificator.set_params(**DTC_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_DTC = f1_score(target_valid, target_predict)\n",
    "print('CrossVal F1', cv_f1_DTC)\n",
    "print('Валидация F1', valid_f1_DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Молодец, что попробовала разные модели в этом шаге!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.7435672514619882\n"
     ]
    }
   ],
   "source": [
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "predict_test = classificator.predict(features_test)\n",
    "\n",
    "print('Test F1:', f1_score(target_test, predict_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Тестирование сделано корректно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"paragraph\">В процессе работы необходимо было обучить модель классифицировать комментарии на позитивные и негативные и построить модель со значением метрики качества <em>F1</em> не меньше 0.75.</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>\n",
    "<div class=\"paragraph\">В нашем распоряжении был набор данных с разметкой о токсичности правок.</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>\n",
    "<div class=\"paragraph\">Столбец <em>text</em>&nbsp;содержит текст комментария, а <em>toxic</em> &mdash; целевой признак.</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>\n",
    "<div class=\"paragraph\">В процессе предобработки проведена лемматизация текстов, очистка от лишних символов, найдены стоп-слова, оценка важности слов определена величиной TF-IDF, дисбаланс классов учтен.</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>\n",
    "<div class=\"paragraph\">Построены модели LogisticRegression и DecisionTreeClassifier, лучшие значения показала модель логистической регрессии.</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>\n",
    "<div class=\"paragraph\">CrossVal F1 0.7525472577805025</div>\n",
    "<div class=\"paragraph\">Валидация F1 0.7574939622105413</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>\n",
    "<div class=\"paragraph\">Значение F1 на тесте 0.7435672514619882</div>\n",
    "<div class=\"paragraph\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Приятно видеть подробный вывод в конце проекта!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 899,
    "start_time": "2023-06-16T09:27:45.792Z"
   },
   {
    "duration": 4880,
    "start_time": "2023-06-16T09:28:26.482Z"
   },
   {
    "duration": 36,
    "start_time": "2023-06-16T09:30:21.623Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-16T09:31:01.257Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-16T09:31:06.563Z"
   },
   {
    "duration": 92,
    "start_time": "2023-06-16T09:31:11.882Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-16T09:31:38.682Z"
   },
   {
    "duration": 858,
    "start_time": "2023-06-16T09:39:38.935Z"
   },
   {
    "duration": 42,
    "start_time": "2023-06-16T09:50:08.985Z"
   },
   {
    "duration": 2677,
    "start_time": "2023-06-16T09:52:40.439Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T09:52:55.879Z"
   },
   {
    "duration": 1861,
    "start_time": "2023-06-16T09:54:24.770Z"
   },
   {
    "duration": 1618,
    "start_time": "2023-06-16T09:55:33.577Z"
   },
   {
    "duration": 3162,
    "start_time": "2023-06-16T09:55:35.197Z"
   },
   {
    "duration": 31,
    "start_time": "2023-06-16T09:55:38.361Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-16T09:55:38.393Z"
   },
   {
    "duration": 45,
    "start_time": "2023-06-16T09:55:38.400Z"
   },
   {
    "duration": 2343,
    "start_time": "2023-06-16T09:55:38.446Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-16T09:55:40.790Z"
   },
   {
    "duration": 2481,
    "start_time": "2023-06-16T09:55:47.658Z"
   },
   {
    "duration": 138,
    "start_time": "2023-06-16T09:56:57.487Z"
   },
   {
    "duration": 70249,
    "start_time": "2023-06-16T09:57:24.607Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-16T10:00:59.709Z"
   },
   {
    "duration": 733,
    "start_time": "2023-06-16T10:00:59.868Z"
   },
   {
    "duration": 36,
    "start_time": "2023-06-16T10:01:00.603Z"
   },
   {
    "duration": 48,
    "start_time": "2023-06-16T10:01:00.640Z"
   },
   {
    "duration": 105,
    "start_time": "2023-06-16T10:01:00.690Z"
   },
   {
    "duration": 98184,
    "start_time": "2023-06-16T10:01:04.673Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T10:02:58.338Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T10:03:17.372Z"
   },
   {
    "duration": 43,
    "start_time": "2023-06-16T10:05:05.820Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-16T10:05:26.294Z"
   },
   {
    "duration": 157,
    "start_time": "2023-06-16T10:08:45.086Z"
   },
   {
    "duration": 1458,
    "start_time": "2023-06-16T10:08:57.485Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-16T10:09:01.668Z"
   },
   {
    "duration": 3171,
    "start_time": "2023-06-16T10:09:01.674Z"
   },
   {
    "duration": 28,
    "start_time": "2023-06-16T10:09:04.847Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-16T10:09:04.877Z"
   },
   {
    "duration": 33,
    "start_time": "2023-06-16T10:09:04.890Z"
   },
   {
    "duration": 81368,
    "start_time": "2023-06-16T10:09:04.924Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-16T10:10:26.294Z"
   },
   {
    "duration": 49,
    "start_time": "2023-06-16T10:10:26.308Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T10:10:26.358Z"
   },
   {
    "duration": 9778,
    "start_time": "2023-06-16T10:10:26.363Z"
   },
   {
    "duration": 143572,
    "start_time": "2023-06-16T10:10:36.143Z"
   },
   {
    "duration": 170,
    "start_time": "2023-06-16T10:15:13.667Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-16T10:15:28.528Z"
   },
   {
    "duration": 130097,
    "start_time": "2023-06-16T10:15:39.514Z"
   },
   {
    "duration": 1400,
    "start_time": "2023-06-16T10:19:27.440Z"
   },
   {
    "duration": 733,
    "start_time": "2023-06-16T10:19:28.842Z"
   },
   {
    "duration": 32,
    "start_time": "2023-06-16T10:19:29.577Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-16T10:19:29.614Z"
   },
   {
    "duration": 42,
    "start_time": "2023-06-16T10:19:29.621Z"
   },
   {
    "duration": 84462,
    "start_time": "2023-06-16T10:19:29.665Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T10:20:54.129Z"
   },
   {
    "duration": 58,
    "start_time": "2023-06-16T10:20:54.137Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-16T10:20:54.196Z"
   },
   {
    "duration": 10621,
    "start_time": "2023-06-16T10:20:54.202Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-16T10:21:04.825Z"
   },
   {
    "duration": 157094,
    "start_time": "2023-06-16T10:21:04.828Z"
   },
   {
    "duration": 136724,
    "start_time": "2023-06-16T10:23:41.926Z"
   },
   {
    "duration": 142696,
    "start_time": "2023-06-16T10:30:09.111Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-16T10:32:44.277Z"
   },
   {
    "duration": 34,
    "start_time": "2023-06-16T10:32:48.465Z"
   },
   {
    "duration": 1134,
    "start_time": "2023-06-16T10:32:52.115Z"
   },
   {
    "duration": 28388,
    "start_time": "2023-06-16T10:33:52.227Z"
   },
   {
    "duration": 40153,
    "start_time": "2023-06-16T10:36:51.647Z"
   },
   {
    "duration": 37862,
    "start_time": "2023-06-16T10:47:15.249Z"
   },
   {
    "duration": 39218,
    "start_time": "2023-06-16T10:49:52.309Z"
   },
   {
    "duration": 11158,
    "start_time": "2023-06-16T10:52:39.257Z"
   },
   {
    "duration": 40737,
    "start_time": "2023-06-16T10:56:53.781Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-16T10:57:45.264Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-16T10:58:37.836Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-16T10:58:56.982Z"
   },
   {
    "duration": 118,
    "start_time": "2023-06-16T11:07:38.602Z"
   },
   {
    "duration": 235,
    "start_time": "2023-06-16T11:08:55.013Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-16T11:09:42.021Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T11:09:49.931Z"
   },
   {
    "duration": 125,
    "start_time": "2023-06-16T11:10:01.922Z"
   },
   {
    "duration": 27741,
    "start_time": "2023-06-16T11:10:15.984Z"
   },
   {
    "duration": 76978,
    "start_time": "2023-06-16T11:11:00.433Z"
   },
   {
    "duration": 281854,
    "start_time": "2023-06-16T11:12:43.214Z"
   },
   {
    "duration": 1587,
    "start_time": "2023-06-16T11:18:05.873Z"
   },
   {
    "duration": 759,
    "start_time": "2023-06-16T11:18:07.462Z"
   },
   {
    "duration": 31,
    "start_time": "2023-06-16T11:18:08.223Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-16T11:18:08.256Z"
   },
   {
    "duration": 35,
    "start_time": "2023-06-16T11:18:08.270Z"
   },
   {
    "duration": 59,
    "start_time": "2023-06-16T11:18:08.307Z"
   },
   {
    "duration": 103747,
    "start_time": "2023-06-16T11:18:08.368Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T11:19:52.117Z"
   },
   {
    "duration": 56,
    "start_time": "2023-06-16T11:19:52.125Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-16T11:19:52.182Z"
   },
   {
    "duration": 10369,
    "start_time": "2023-06-16T11:19:52.188Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T11:20:02.559Z"
   },
   {
    "duration": 106845,
    "start_time": "2023-06-16T11:20:02.564Z"
   },
   {
    "duration": 77805,
    "start_time": "2023-06-16T11:21:49.413Z"
   },
   {
    "duration": 60292,
    "start_time": "2023-06-16T11:23:07.220Z"
   },
   {
    "duration": 24,
    "start_time": "2023-06-16T11:24:07.514Z"
   },
   {
    "duration": 1115,
    "start_time": "2023-06-16T11:24:07.540Z"
   },
   {
    "duration": 28257,
    "start_time": "2023-06-16T11:24:08.657Z"
   },
   {
    "duration": 39493,
    "start_time": "2023-06-16T11:24:36.915Z"
   },
   {
    "duration": 33002,
    "start_time": "2023-06-16T11:25:16.410Z"
   },
   {
    "duration": 11394,
    "start_time": "2023-06-16T11:25:49.413Z"
   },
   {
    "duration": 16,
    "start_time": "2023-06-16T11:26:00.810Z"
   },
   {
    "duration": 302521,
    "start_time": "2023-06-16T11:26:00.828Z"
   },
   {
    "duration": 4990,
    "start_time": "2023-06-16T11:31:20.037Z"
   },
   {
    "duration": 1870,
    "start_time": "2023-06-16T12:27:50.368Z"
   },
   {
    "duration": 937,
    "start_time": "2023-06-16T12:27:52.241Z"
   },
   {
    "duration": 47,
    "start_time": "2023-06-16T12:27:53.180Z"
   },
   {
    "duration": 20,
    "start_time": "2023-06-16T12:27:53.236Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-16T12:27:53.258Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T12:27:53.276Z"
   },
   {
    "duration": 53,
    "start_time": "2023-06-16T12:27:53.280Z"
   },
   {
    "duration": 110704,
    "start_time": "2023-06-16T12:27:53.335Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-16T12:29:44.047Z"
   },
   {
    "duration": 69,
    "start_time": "2023-06-16T12:29:44.058Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-16T12:29:44.128Z"
   },
   {
    "duration": 11544,
    "start_time": "2023-06-16T12:29:44.135Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T12:29:55.681Z"
   },
   {
    "duration": 112352,
    "start_time": "2023-06-16T12:29:55.688Z"
   },
   {
    "duration": 83202,
    "start_time": "2023-06-16T12:31:48.042Z"
   },
   {
    "duration": 72210,
    "start_time": "2023-06-16T12:33:11.247Z"
   },
   {
    "duration": 82,
    "start_time": "2023-06-16T12:34:23.459Z"
   },
   {
    "duration": 1327,
    "start_time": "2023-06-16T12:34:23.543Z"
   },
   {
    "duration": 39550,
    "start_time": "2023-06-16T12:34:24.871Z"
   },
   {
    "duration": 48307,
    "start_time": "2023-06-16T12:35:04.435Z"
   },
   {
    "duration": 35570,
    "start_time": "2023-06-16T12:35:52.749Z"
   },
   {
    "duration": 12387,
    "start_time": "2023-06-16T12:36:28.321Z"
   },
   {
    "duration": 29,
    "start_time": "2023-06-16T12:36:40.710Z"
   },
   {
    "duration": 606892,
    "start_time": "2023-06-16T12:36:40.746Z"
   },
   {
    "duration": 44014,
    "start_time": "2023-06-16T12:50:38.900Z"
   },
   {
    "duration": 13765,
    "start_time": "2023-06-16T12:52:58.705Z"
   },
   {
    "duration": 304271,
    "start_time": "2023-06-16T12:55:15.071Z"
   },
   {
    "duration": 18623,
    "start_time": "2023-06-16T13:01:19.499Z"
   },
   {
    "duration": 1510,
    "start_time": "2023-06-16T13:10:55.030Z"
   },
   {
    "duration": 811,
    "start_time": "2023-06-16T13:10:56.549Z"
   },
   {
    "duration": 29,
    "start_time": "2023-06-16T13:10:57.362Z"
   },
   {
    "duration": 34,
    "start_time": "2023-06-16T13:10:57.393Z"
   },
   {
    "duration": 21,
    "start_time": "2023-06-16T13:10:57.429Z"
   },
   {
    "duration": 23,
    "start_time": "2023-06-16T13:10:57.452Z"
   },
   {
    "duration": 66,
    "start_time": "2023-06-16T13:10:57.477Z"
   },
   {
    "duration": 92549,
    "start_time": "2023-06-16T13:10:57.544Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-16T13:12:30.095Z"
   },
   {
    "duration": 87,
    "start_time": "2023-06-16T13:12:30.103Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T13:12:30.192Z"
   },
   {
    "duration": 11077,
    "start_time": "2023-06-16T13:12:30.196Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T13:12:41.275Z"
   },
   {
    "duration": 578145,
    "start_time": "2023-06-16T13:12:41.280Z"
   },
   {
    "duration": 49098,
    "start_time": "2023-06-16T13:22:19.427Z"
   },
   {
    "duration": 379376,
    "start_time": "2023-06-16T13:23:08.530Z"
   },
   {
    "duration": 23725,
    "start_time": "2023-06-16T13:29:27.917Z"
   },
   {
    "duration": 49677,
    "start_time": "2023-06-16T13:44:16.120Z"
   },
   {
    "duration": 48812,
    "start_time": "2023-06-16T13:47:05.877Z"
   },
   {
    "duration": 161,
    "start_time": "2023-06-16T13:51:50.001Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-16T13:55:51.299Z"
   },
   {
    "duration": 1896,
    "start_time": "2023-06-16T13:59:07.370Z"
   },
   {
    "duration": 945,
    "start_time": "2023-06-16T13:59:09.268Z"
   },
   {
    "duration": 36,
    "start_time": "2023-06-16T13:59:10.215Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-16T13:59:10.253Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-16T13:59:10.269Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-16T13:59:10.283Z"
   },
   {
    "duration": 48,
    "start_time": "2023-06-16T13:59:10.292Z"
   },
   {
    "duration": 131262,
    "start_time": "2023-06-16T13:59:10.342Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-16T14:01:21.606Z"
   },
   {
    "duration": 59,
    "start_time": "2023-06-16T14:01:21.619Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T14:01:21.680Z"
   },
   {
    "duration": 14040,
    "start_time": "2023-06-16T14:01:21.685Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-16T14:01:35.727Z"
   },
   {
    "duration": 953719,
    "start_time": "2023-06-16T14:01:35.731Z"
   },
   {
    "duration": 49233,
    "start_time": "2023-06-16T14:17:29.521Z"
   },
   {
    "duration": 689382,
    "start_time": "2023-06-16T14:18:18.756Z"
   },
   {
    "duration": 24546,
    "start_time": "2023-06-16T14:29:48.140Z"
   },
   {
    "duration": 51536,
    "start_time": "2023-06-16T14:30:12.688Z"
   },
   {
    "duration": 2305,
    "start_time": "2023-06-17T07:00:51.942Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-17T07:02:23.384Z"
   },
   {
    "duration": 3393,
    "start_time": "2023-06-17T07:02:27.545Z"
   },
   {
    "duration": 44,
    "start_time": "2023-06-17T07:02:34.034Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-17T07:02:35.943Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-17T07:02:37.061Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-17T07:02:40.136Z"
   },
   {
    "duration": 42,
    "start_time": "2023-06-17T07:02:42.431Z"
   },
   {
    "duration": 8787,
    "start_time": "2023-06-17T07:02:48.523Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-17T07:06:22.926Z"
   },
   {
    "duration": 174,
    "start_time": "2023-06-17T07:16:54.115Z"
   },
   {
    "duration": 1622,
    "start_time": "2023-06-17T07:17:44.256Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-17T07:17:58.721Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-17T07:18:00.576Z"
   },
   {
    "duration": 1642,
    "start_time": "2023-06-17T07:18:02.025Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-17T07:18:28.250Z"
   },
   {
    "duration": 874,
    "start_time": "2023-06-17T07:18:28.265Z"
   },
   {
    "duration": 32,
    "start_time": "2023-06-17T07:18:29.142Z"
   },
   {
    "duration": 20,
    "start_time": "2023-06-17T07:18:29.175Z"
   },
   {
    "duration": 24,
    "start_time": "2023-06-17T07:18:29.196Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-17T07:18:29.223Z"
   },
   {
    "duration": 50,
    "start_time": "2023-06-17T07:18:29.230Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-17T07:18:29.281Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-17T07:18:29.289Z"
   },
   {
    "duration": 4947,
    "start_time": "2023-06-17T07:18:38.582Z"
   },
   {
    "duration": 84776,
    "start_time": "2023-06-17T07:18:46.118Z"
   },
   {
    "duration": 66,
    "start_time": "2023-06-17T07:22:04.665Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-17T07:22:22.338Z"
   },
   {
    "duration": 47,
    "start_time": "2023-06-17T07:22:29.937Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-17T07:22:31.054Z"
   },
   {
    "duration": 7780,
    "start_time": "2023-06-17T07:22:38.575Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-17T07:24:14.982Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-17T07:24:52.551Z"
   },
   {
    "duration": 128727,
    "start_time": "2023-06-17T07:24:53.930Z"
   },
   {
    "duration": 2409,
    "start_time": "2023-06-17T07:27:14.658Z"
   },
   {
    "duration": 1244,
    "start_time": "2023-06-17T07:27:17.070Z"
   },
   {
    "duration": 50,
    "start_time": "2023-06-17T07:27:18.316Z"
   },
   {
    "duration": 18,
    "start_time": "2023-06-17T07:27:18.368Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-17T07:27:18.388Z"
   },
   {
    "duration": 51,
    "start_time": "2023-06-17T07:27:18.420Z"
   },
   {
    "duration": 73,
    "start_time": "2023-06-17T07:27:18.473Z"
   },
   {
    "duration": 25,
    "start_time": "2023-06-17T07:27:18.547Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-17T07:27:18.574Z"
   },
   {
    "duration": 6084,
    "start_time": "2023-06-17T07:27:18.586Z"
   },
   {
    "duration": 93038,
    "start_time": "2023-06-17T07:27:24.671Z"
   },
   {
    "duration": 50,
    "start_time": "2023-06-17T07:28:57.711Z"
   },
   {
    "duration": 32,
    "start_time": "2023-06-17T07:28:57.762Z"
   },
   {
    "duration": 37,
    "start_time": "2023-06-17T07:28:57.809Z"
   },
   {
    "duration": 7208,
    "start_time": "2023-06-17T07:28:57.847Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-17T07:29:05.056Z"
   },
   {
    "duration": 549816,
    "start_time": "2023-06-17T07:29:05.060Z"
   },
   {
    "duration": 27203,
    "start_time": "2023-06-17T07:38:14.878Z"
   },
   {
    "duration": 359277,
    "start_time": "2023-06-17T07:38:42.083Z"
   },
   {
    "duration": 21156,
    "start_time": "2023-06-17T07:44:41.361Z"
   },
   {
    "duration": 28239,
    "start_time": "2023-06-17T07:45:02.518Z"
   },
   {
    "duration": 118,
    "start_time": "2023-06-17T07:58:32.834Z"
   },
   {
    "duration": 1895,
    "start_time": "2023-06-18T09:13:23.505Z"
   },
   {
    "duration": 2434,
    "start_time": "2023-06-18T09:13:25.402Z"
   },
   {
    "duration": 33,
    "start_time": "2023-06-18T09:13:27.838Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-18T09:13:27.873Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-18T09:13:27.882Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-18T09:13:27.892Z"
   },
   {
    "duration": 66,
    "start_time": "2023-06-18T09:13:27.898Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-18T09:13:27.965Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-18T09:17:47.565Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:17:49.352Z"
   },
   {
    "duration": 4434,
    "start_time": "2023-06-18T09:17:52.445Z"
   },
   {
    "duration": 361,
    "start_time": "2023-06-18T09:17:56.881Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-18T09:18:58.919Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:20:35.325Z"
   },
   {
    "duration": 3708,
    "start_time": "2023-06-18T09:20:36.207Z"
   },
   {
    "duration": 267,
    "start_time": "2023-06-18T09:20:39.917Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-18T09:21:28.538Z"
   },
   {
    "duration": 3714,
    "start_time": "2023-06-18T09:21:29.220Z"
   },
   {
    "duration": 339,
    "start_time": "2023-06-18T09:21:33.874Z"
   },
   {
    "duration": 495,
    "start_time": "2023-06-18T09:22:11.114Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:22:23.026Z"
   },
   {
    "duration": 4045,
    "start_time": "2023-06-18T09:22:24.212Z"
   },
   {
    "duration": 444064,
    "start_time": "2023-06-18T09:22:28.260Z"
   },
   {
    "duration": 1679,
    "start_time": "2023-06-18T09:30:06.593Z"
   },
   {
    "duration": 827,
    "start_time": "2023-06-18T09:30:08.274Z"
   },
   {
    "duration": 38,
    "start_time": "2023-06-18T09:30:09.102Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-18T09:30:09.142Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-18T09:30:09.149Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-18T09:30:09.158Z"
   },
   {
    "duration": 60,
    "start_time": "2023-06-18T09:30:09.167Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:30:09.232Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-18T09:30:09.237Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-18T09:30:09.254Z"
   },
   {
    "duration": 4450,
    "start_time": "2023-06-18T09:30:09.268Z"
   },
   {
    "duration": 524,
    "start_time": "2023-06-18T09:30:13.720Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.246Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.248Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.249Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.250Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.251Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.252Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.253Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.254Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.255Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:30:14.256Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-18T09:32:05.210Z"
   },
   {
    "duration": 743,
    "start_time": "2023-06-18T09:32:14.995Z"
   },
   {
    "duration": 31,
    "start_time": "2023-06-18T09:32:16.628Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-18T09:32:17.837Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-18T09:32:20.097Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-18T09:32:22.519Z"
   },
   {
    "duration": 34,
    "start_time": "2023-06-18T09:32:24.948Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:32:26.958Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:32:29.375Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:32:30.398Z"
   },
   {
    "duration": 4518,
    "start_time": "2023-06-18T09:32:31.525Z"
   },
   {
    "duration": 269,
    "start_time": "2023-06-18T09:32:37.840Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-18T09:33:19.691Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:33:37.195Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:33:38.006Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T09:33:38.891Z"
   },
   {
    "duration": 3601,
    "start_time": "2023-06-18T09:33:39.794Z"
   },
   {
    "duration": 191765,
    "start_time": "2023-06-18T09:33:46.317Z"
   },
   {
    "duration": 1618,
    "start_time": "2023-06-18T09:37:07.958Z"
   },
   {
    "duration": 824,
    "start_time": "2023-06-18T09:37:09.577Z"
   },
   {
    "duration": 32,
    "start_time": "2023-06-18T09:37:10.402Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-18T09:37:10.436Z"
   },
   {
    "duration": 16,
    "start_time": "2023-06-18T09:37:10.443Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-18T09:37:10.460Z"
   },
   {
    "duration": 53,
    "start_time": "2023-06-18T09:37:10.476Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-18T09:37:10.531Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-18T09:37:10.546Z"
   },
   {
    "duration": 47,
    "start_time": "2023-06-18T09:37:10.557Z"
   },
   {
    "duration": 4466,
    "start_time": "2023-06-18T09:37:10.606Z"
   },
   {
    "duration": 631624,
    "start_time": "2023-06-18T09:37:15.074Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.700Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.701Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.702Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.703Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.704Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.705Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.707Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.708Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.709Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-18T09:47:46.711Z"
   },
   {
    "duration": 1707,
    "start_time": "2023-06-18T10:02:47.151Z"
   },
   {
    "duration": 804,
    "start_time": "2023-06-18T10:02:48.860Z"
   },
   {
    "duration": 29,
    "start_time": "2023-06-18T10:02:49.665Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-18T10:02:49.697Z"
   },
   {
    "duration": 29,
    "start_time": "2023-06-18T10:02:49.705Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-18T10:02:49.735Z"
   },
   {
    "duration": 45,
    "start_time": "2023-06-18T10:02:49.739Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-18T10:02:49.786Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-18T10:02:49.790Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-18T10:02:49.799Z"
   },
   {
    "duration": 4755,
    "start_time": "2023-06-18T10:02:49.805Z"
   },
   {
    "duration": 1016322,
    "start_time": "2023-06-18T10:02:54.563Z"
   },
   {
    "duration": 55,
    "start_time": "2023-06-18T10:19:50.887Z"
   },
   {
    "duration": 36,
    "start_time": "2023-06-18T10:19:50.943Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-18T10:19:50.980Z"
   },
   {
    "duration": 5888,
    "start_time": "2023-06-18T10:19:50.985Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-18T10:19:56.875Z"
   },
   {
    "duration": 630156,
    "start_time": "2023-06-18T10:19:56.878Z"
   },
   {
    "duration": 42497,
    "start_time": "2023-06-18T10:30:27.036Z"
   },
   {
    "duration": 320266,
    "start_time": "2023-06-18T10:31:09.534Z"
   },
   {
    "duration": 25601,
    "start_time": "2023-06-18T10:36:29.803Z"
   },
   {
    "duration": 40936,
    "start_time": "2023-06-18T10:36:55.406Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
